# 四 集群搭建
## Hadoop集群搭建
```xml
# ip
vi /etc/sysconfig/network-scripts/ifcfg-ens33
	IPADDR=192.168.184.101
	NETMASK=255.255.255.0
	GATEWAY=192.168.184.1
	DNS1=192.168.184.1

# 域名解析
vi /etc/hosts
	192.168.184.101 hadoop-master
	192.168.184.102 hadoop-slave1
	192.168.184.103 hadoop-slave2
vi /etc/hostname ...

# 下载并解压 ...

# 环境变量
vi /etc/profile
  export JAVA_HOME=/home/wesharecn/jdk1.8.0_311
  export HADOOP_HOME=/home/wesharecn/hadoop-2.7.7
  export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

# hadoop配置
vi $HADOOP_HOME/etc/hadoop
  hadoop-env.sh ...
  slaves ...

  1.core-site.xml
    <configuration>
        <property>
            <name>fs.defaultFS</name>
            <value>hdfs://hadoop-master:8020</value>
        </property>
        <property>
            <name>hadoop.tmp.dir</name>
            <value>/home/wesharecn/hadoop</value>
        </property>
    </configuration>

  2.hdfs-site.xml
    <configuration>
      	<property>
            <name>dfs.namenode.secondary.http-address</name>
            <value>hadoop-slave1:50090</value>
    		</property>
    </configuration>

  3.mapred-site.xml
    <configuration>
        <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
        </property>
    </configuration>

  4.yarn-site.xml
    <configuration>
        <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
        </property>
        <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>hadoop-slave2</value>
        </property>
        <!-- <property>
            <name>yarn.scheduler.maximum-allocation-mb</name>
            <value>2048</value>
        </property>
				<property>
            <name>yarn.scheduler.maximum-allocation-vcores</name>
            <value>1</value>
        </property> -->
    </configuration>

service firewalld stop（systemctl disable firewalld.service）

# 克隆虚拟机&免密登录（ip、hostname）...
ssh-keygen -t rsa
ssh-copy-id hadoop-master/hadoop-slave1/hadoop-slave2

# 格式化并启动集群
hadoop namenode -format
start-all.sh（start-dfs.sh、start-yarn.sh）
http://hadoop-master:50070 http://hadoop-slave2:8088

# MR测试
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar pi 2 2
```

> **注意：NN和RM如果不是同一台机器，应该在RM所在的机器上启动yarn。**

### 查看进程
vi bin/xcall

```shell
#!/bin/bash
for node in hadoop-master hadoop-slave1 hadoop-slave2
do
	echo ---- $node ----
  ssh $node "source /etc/profile; $*"
done
```

chmod 770 bin/xcall

**xcall jps**

```tex
---- hadoop-master ----
5300 NameNode
10142 NodeManager
5487 DataNode
---- hadoop-slave1 ----
3524 SecondaryNameNode
5883 NodeManager
3405 DataNode
---- hadoop-slave2 ----
4997 NodeManager
4838 ResourceManager
2968 DataNode
```

## Zookeeper集群搭建

```shell
#环境变量
vi /etc/profile
	export ZK_HOME=/home/wesharecn/zookeeper-3.5.7
	export PATH=$PATH:$ZK_HOME/bin
  
vi $ZK_HOME/conf/zoo.cfg
	dataDir=/home/wesharecn/zookeeper-3.5.7/zkData
	
	#######################cluster##########################
	server.1=hadoop-master:2888:3888
	server.2=hadoop-slave1:2888:3888
	server.3=hadoop-slave2:2888:3888

# 集群分发
scp -r zookeeper-3.5.7 hadoop-slave1:$PWD
scp -r zookeeper-3.5.7 hadoop-slave2:$PWD
sudo scp /etc/profile hadoop-slave1:/etc
sudo scp /etc/profile hadoop-slave2:/etc
xcall source /etc/profile

# 修改为每台设备唯一
vi zkData/myid ...

# 启动ZK集群
xcall zkServer.sh restart|status
zkCli.sh -server hadoop-master
```
### 选举机制
>**第一次选举**
>
>（1）服务器1启动，发起一次选举。服务器1投自己一票。此时服务器1票数一票，不够半数以上（3票），选举无法完成，服务器1状态保持为LOOKING； 
>（2）服务器2启动，再发起一次选举。服务器1和2分别投自己一票并交换选票信息：此时服务器1发现服务器2的myid比自己目前投票推举的（服务器1） 大，更改选票为推举服务器2。此时服务器1票数0票，服务器2票数2票，没有半数以上结果，选举无法完成，服务器1，2状态保持LOOKING；
>（3）服务器3启动，发起一次选举。此时服务器1和2都会更改选票为服务器3。此次投票结果：服务器1为0票，服务器2为0票，服务器3为3票。此时服务器3的票数已经超过半数，服务器3当选Leader。服务器1，2更改状态为FOLLOWING，服务器3更改状态为LEADING；
>（4）服务器4启动，发起一次选举。此时服务器1，2，3已经不是LOOKING状态，不会更改选票信息。交换选票信息结果：服务器3为3票，服务器4为 1票。此时服务器4服从多数，更改选票信息为服务器3，并更改状态为FOLLOWING； 
>（5）服务器5启动，同4一样当小弟。
>
>**非第一次选举**
>
>SID：服务器ID。
>ZXID：事务ID。
>Epoch：每个Leader任期的代号。没有Leader时同一轮投票过程中的逻辑时钟值是相同的。每投完一次票这个数据就会增加
>
>选举Leader规则： ①EPOCH大的直接胜出 ②EPOCH相同，ZXID大的胜出 ③ZXID相同，SID大的胜出

## HBase集群搭建
```xml
# 解压
curl -O http://archive.apache.org/dist/hbase/1.7.1 && tar -zxvf hbase-1.7.1-bin.tar.gz

# 环境变量
sudo vi /etc/profile
	export HBASE_HOME=/home/wesharecn/hbase-1.7.1
	export PATH=$PATH:${HBASE_HOME}/bin

# hbase配置
vi $HBASE_HOME/conf
		regionservers ...
		hbase-env.sh
			. /etc/profile
			export JAVA_HOME=${JAVA_HOME}
			export HBASE_CLASSPATH=${HADOOP_HOME}/etc/hadoop
			export HBASE_MANAGES_ZK=FALSE

	 	hbase-site.sh
			<configuration>
				<property>
					<name>hbase.rootdir</name>
					<value>hdfs://hadoop-master:8020/hbase</value>
				</property>
				<property>
					<name>hbase.cluster.distributed</name>
					<value>true</value>
				</property>
				<property>
					<name>hbase.zookeeper.quorum</name>
					<value>hadoop-master,hadoop-slave1,hadoop-slave2</value>
				</property>
			</configuration>
```
```shell
# 替换jar

# 集群分发
scp -r hbase-0.98.24 hadoop-slave1:$PWD
scp -r hbase-0.98.24 hadoop-slave2:$PWD
sudo scp /etc/profile hadoop-slave1:/etc
sudo scp /etc/profile hadoop-slave2:/etc
xcall source /etc/profile

# 启动集群
hdfs... zookeeper... start-hbase.sh http://hadoop-master:16010
```
>Web UI 端口更改
>在0.98.x以上的HBase中，HBase Web UI使用的HTTP端口从Master的60010和每个RegionServer的60030变为Master的16010和RegionServer的16030。

### shell
```shell
[wesharecn@hadoop-master ~]$ hbase shell
hbase(main):001:0> status
1 active master, 0 backup masters, 3 servers, 3 dead, 0.6667 average load

hbase(main):002:0> create 'user_info', 'basic_info', 'ext_info'
hbase(main):003:0> put 'user_info', '1', 'basic_info:username', '张三'
hbase(main):004:0> put 'user_info', '1', 'basic_info:password', 'pwd'
hbase(main):005:0> put 'user_info', '1', 'ext_info:age', '18'
hbase(main):006:0> put 'user_info', '1', 'ext_info:gender', 'male'
hbase(main):007:0> put 'user_info', '1', 'ext_info:addr', '上海'

hbase(main):008:0> get 'user_info', '1'
COLUMN                           CELL                                                                                        
 basic_info:password             timestamp=1638236880946, value=P@ssw0rd                                                     
 basic_info:username             timestamp=1638236829350, value=zhangsan                                                     
 ext_info:addr                   timestamp=1638236966441, value=shanghai                                                     
 ext_info:age                    timestamp=1638236914445, value=18                                                           
 ext_info:gender                 timestamp=1638236949576, value=male                                                         
5 row(s) in 0.0410 seconds

scan 'user_info',{FILTER=>"RowFilter(=,'binary:1')", COLUMNS=>['basic_info:name','ext_info:addr', 'ext_info:age'], FORMATTER=>'toString'}
```

## Kafka集群搭建
```properties
# 环境变量
vi /etc/profile
	export KAFKA_HOME=/home/wesharecn/kafka_2.11-2.4.1
	export PATH=$PATH:$KAFKA_HOME/bin
  
vi kafka_2.11-2.4.1/config/server.properties
	#修改为每台设备唯一
	broker.id=0
	delete.topic.enable=true
	zookeeper.connect=hadoop-master:2181,hadoop-slave1:2181,hadoop-slave2:2181/kafka

# 集群分发
scp -r kafka_2.11-2.4.1 hadoop-slave1:$PWD
scp -r kafka_2.11-2.4.1 hadoop-slave2:$PWD
sudo scp /etc/profile hadoop-slave1:/etc
sudo scp /etc/profile hadoop-slave2:/etc
xcall source /etc/profile

# 启动Kafka集群
xcall kafka-server-start.sh -daemon kafka_2.11-2.4.1/config/server.properties
```

### shell

 ```shell
 # 查看topic列表
 kafka-topics.sh --zookeeper hadoop-master/kafka --list
 # 查看topic详情
 kafka-topics.sh --zookeeper hadoop-master/kafka --describe --topic topic_log
 # 创建topic
 kafka-topics.sh --zookeeper hadoop-master,hadoop-slave1,hadoop-slave2/kafka --create --replication-factor 1 --partitions 1 --topic topic_log
 # 删除topic
 kafka-topics.sh --zookeeper hadoop-master,hadoop-slave1,hadoop-slave2/kafka --delete --topic topic_log
 # 查看历史消息
 kafka-console-consumer.sh --bootstrap-server hadoop-master:9092 --from-beginning --topic topic_log
 
 # 压测
 kafka-producer-perf-test.sh --topic test --record-size 100 --num-records 10000000 --throughput -1 --producer-props bootstrap.servers=hadoop-master:9092,hadoop-slave1:9092,hadoop-slave2:9092
 kafka-consumer-perf-test.sh --topic test --fetch-size 10000 --messages 10000000 --threads 1 --broker-list hadoop-master:9092,hadoop-slave1:9092,hadoop-slave2:9092
 ```

## Hive环境搭建
```xml
# 解压
curl -O https://archive.apache.org/dist/hive/hive-2.3.9/apache-hive-2.3.9-bin.tar.gz && tar -zxvf apache-hive-2.3.9-bin.tar.gz

# 环境变量
sudo vi /etc/profile
  export HIVE_HOME=/home/wesharecn/apache-hive-2.3.9-bin
  export PATH=$PATH:${HIVE_HOME}/bin
source /etc/profile

# hive配置
vi $HIVE_HOME/conf/hive-site.xml
		<?xml version="1.0"?>
		<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>javax.jdo.option.ConnectionURL</name>
            <value>jdbc:mysql://192.168.12.101:3306/metastore1?useSSL=false</value>
        </property>
        <property>
            <name>javax.jdo.option.ConnectionDriverName</name>
            <value>com.mysql.jdbc.Driver</value>
        </property>
        <property>
            <name>javax.jdo.option.ConnectionUserName</name>
            <value>root</value>
        </property>
        <property>
            <name>javax.jdo.option.ConnectionPassword</name>
            <value>root123</value>
        </property>
        <property>
            <name>hive.metastore.warehouse.dir</name>
            <value>/user/hive/warehouse</value>
        </property>
        <property>
            <name>hive.metastore.schema.verification</name>
            <value>false</value>
        </property>
        <property>
        <name>hive.server2.thrift.port</name>
        <value>10000</value>
        </property>
        <property>
            <name>hive.server2.thrift.bind.host</name>
            <value>hadoop-slave1</value>
        </property>
        <property>
            <name>hive.metastore.event.db.notification.api.auth</name>
            <value>false</value>
        </property>
        <property>
            <name>hive.cli.print.header</name>
            <value>true</value>
        </property>
        <property>
            <name>hive.cli.print.current.db</name>
            <value>true</value>
        </property>
    </configuration>
    
# 准备驱动
cp mysql-connector-java-5.1.27-bin.jar $HIVE_HOME/lib

# 登录MySQL
create database metastore1 ...
	
# 初始化(可能有异常，请自行排查)
schematool -initSchema -dbType mysql -verbose
```
### shell
```sql
show databases;

create table t1(id int, name string, hobby array<string>, addr map<string, string>) row format delimited fields terminated by ";" collection items terminated by "," map keys terminated by ":";

load data local inpath '/home/wesharecn/t1.dat' into table t1; 
alter table t1 set tblproperties('EXTERNAL'='FALSE');

create table if not exists t3( id int ,name string ,hobby array<string> ,addr map<String,string> ) partitioned by (dt string) row format delimited fields terminated by ';' collection items terminated by ',' map keys terminated by ':';

load data local inpath "/home/wesharecn/t1.dat" into table t3 partition(dt="2020-06-01");
```


## Hive on Spark环境搭建

```shell
# 解压
curl -O https://archive.apache.org/dist/spark/spark-2.0.0/spark-2.0.0-bin-hadoop2.7.tgz && tar -zxvf spark-2.0.0-bin-hadoop2.7.tgz

# 环境变量
sudo vi /etc/profile
	export SPARK_HOME=/home/wesharecn/spark-2.0.0-bin-hadoop2.7
	export PATH=$PATH:${SPARK_HOME}/bin
source /etc/profile
	
# hive配置
vi $HIVE_HOME/conf/spark-defaults.conf
  spark.master	yarn
  spark.eventLog.enabled	true
  spark.eventLog.dir	hdfs://hadoop-master:8020/spark-history
  spark.executor.memory	1g
  spark.driver.memory	1g

# 历史日志
hadoop fs -mkdir /spark-history

# 上传jar（cp $HIVE_HOME/lib/parquet-hadoop-bundle-1.8.1.jar $SPARK_HOME/jars）
hadoop fs -mkdir /spark-jars
hadoop fs -put $SPARK_HOME/jars/* /spark-jars（spark-xxx-bin-without-hadoop）


# hive配置
vi $HIVE_HOME/conf/hive-site.xml
  <property>
      <name>spark.yarn.jars</name>
      <value>hdfs://hadoop-master:8020/spark-jars/*</value>
  </property>
  <property>
      <name>hive.execution.engine</name>
      <value>spark</value>
  </property>
```

### 测试

```sql
# 创建一张测试表
create table student(id INT);
insert into table student values(1);

CREATE TABLE u_data (
  userid INT,
  movieid INT,
  rating INT,
  unixtime STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;

wget http://files.grouplens.org/datasets/movielens/ml-100k.zip && unzip ml-100k.zip
LOAD DATA LOCAL INPATH '/home/wesharecn/ml-100k/u.data' OVERWRITE INTO TABLE u_data;
SELECT COUNT(*) FROM u_data;
```

> 遇坑指南：
> hive --hiveconf hive.root.logger=INFO,console  -e  "select count(*) from student"
> https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started