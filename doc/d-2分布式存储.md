## HDFS
### Hadoop介绍

Hadoop最早起源于Nutch。Nutch的设计目标是构建一个大型的全网搜索引擎，包括网页抓取、索引、查询等功能，但随着抓取网页数量的增加，遇到了严重的可扩展性问题——如何解决数十亿网页的存储和索引问题。

2003年、2004年谷歌发表的两篇论文为该问题提供了可行的解决方案：

- 分布式文件系统（GFS），可用于处理海量网页的存储
- 分布式计算框架MAPREDUCE，可用于处理海量网页的索引计算问题。

Nutch的开发人员完成了相应的开源实现HDFS和MAPREDUCE，并从Nutch中剥离成为独立项目HADOOP，到2008年1月，HADOOP成为Apache顶级项目(同年，cloudera公司成立)，迎来了它的快速发展期。

狭义上来说，hadoop就是单独指代hadoop这个软件，
广义上来说，hadoop指代大数据的一个生态圈，包括很多其他的软件

### 概述

HDFS：Hadoop Distributed File System

2003年10月Google发布了GFS。

HDFS是GFS的开源实现，Java语言开发

HDFS是Apache Hadoop的核心子项目

HDFS是大数据技术体系的基石，地位无可替代

### 硬件环境

| 年份         | 2003         | 2006           | 2010         | 2015                 |
| ------------ | ------------ | -------------- | ------------ | -------------------- |
| 磁盘容量     | 80G          | 250G           | 1T           | 4T                   |
| 磁盘总线带宽 | 66M/s(ATA66) | 100M/s(ATA100) | 150M/s(SATA) | 800M/S (PCI-Express) |
| HDD读写速度  | 66/66        | 100/70         | 120/70       | 120/70               |
| SSD读写速度  |              |                | 480/260      | 800/400              |

T(存储时间)=T(定位时间)+T(传输时间)

### 设计目标
● 运行与大量廉价商业机器，硬件错误是常态，需要提供容错机制

● 简化的一致性模型：一次写入多次读取，支持追加，不允许修改，保证数据一致性

● 顺序读取而非随机读取，关注吞吐而非延迟

● 存储大规模数据集： 通常情况下在HDFS上运行的应用都具有大的数据集，一个典型的 HDFS 文件大小是 GB 到 TB 的级别，在实际中超大型集群的HDFS中文件的大小也可能达到PB级别

● 可移植性：为了方便HDFS作为大规模数据应用平台得到广泛应用，HDFS被设计成了一个易于从一个平台移植到另一个平台的数据应用系统

● 移动计算的位置比移动数据的位置更划算。由于网络带宽是一个有限资源，一个计算任务的运行，当计算数据离计算逻辑所在节点越近这个计算任务执行的就越高效，数据量越大这种特点就越明显。所以在 HDFS 中设计了提供给应用将计算逻辑移动到数据附近这样的接口

### 系统架构

![img](https://cdn.nlark.com/yuque/0/2021/png/23034366/1638675440760-10162faf-591e-46ee-a384-5cfa587a789c.png)

#### 架构中的角色

- Active NameNode（AN）

- - 活动Master管理节点（集群中唯一）
  - 管理命名空间•管理元数据：文件的位置、所有者、权限、数据块等

- - 管理Block副本策略：默认3个副本
  - 处理客户端读写请求，为DataNode分配任务

- Standby NameNode（SN）

- - 热备Master管理节点（Active NameNode的热备节点）Hadoop 3.x允许配置多个Standby NameNode
  - Active NameNode宕机后，快速升级为新的Active

- - 周期性同步edits编辑日志，定期合并fsimage与edits到本地磁盘

备注：Hadoop 1.x版本使用Secondary NameNode， Hadoop 2.x版本使用 Standby NameNode

![img](https://cdn.nlark.com/yuque/0/2021/png/23034366/1638677009446-28dce2cc-7aed-44bc-b661-60bd9a6875ed.png)

#### NameNode元数据文件

- edits（编辑日志文件）：保存了自最新检查点（Checkpoint）之后的所有文件更新操作
- fsimage（元数据检查点镜像文件）：保存了文件系统中所有的目录和文件信息，如：某个目录下有哪些子目录和文件，以及文件名、文件副本数、文件由哪些Block组成等

- Active NameNode内存中有一份最新的元数据（= fsimage + edits）
- Standby NameNode在检查点定期将内存中的元数据保存到fsimage文件中

#### DataNode
● Slave工作节点（可大规模扩展）

● 存储Block和数据校验和

● 执行客户端发送的读写操作

● 通过心跳机制定期（默认3秒）向NameNode汇报运行状态和Block列表信息

● 集群启动时，DataNode向NameNode提供Block列表信息

#### Block数据块
● HDFS最小存储单元

● 文件写入HDFS会被切分成若干个Block

● Block和元数据分开存储：Block存储于DataNode，元数据存储于NameNode

● Block大小固定，默认为128MB，可自定义(最初为64MB)

● 若一个Block的大小小于设定值，不会占用整个块空间

● 默认情况下每个Block有3个副本
  ○ 机架感知：将副本存储到不同的机架上，实现数据的高容错
  ○ 副本均匀分布：提高访问带宽和读取性能，实现负载均衡

● Block文件是DataNode本地磁盘中名为“blk_blockId”的Linux文件。DataNode在启动时自动创建存储目录，无需格式化。DataNode的current目录下的文件名都以“blk_”为前缀。Block元数据文件（*.meta）由一个包含版本、类型信息的头文件和一系列校验值组成。

Block副本放置策略

● 副本1：放在Client所在节点
  ○ 对于远程Client，系统会随机选择节点
  ○ 系统会首先选择空闲的DataNode节点

● 副本2：放在不同的机架节点上

● 副本3：放在与第二个副本同一机架的不同节点上

● 副本N：随机选择

● 节点选择：同等条件下优先选择空闲节点

#### Client
● 将文件切分为Block

● 与NameNode交互，获取文件元数据

● 与DataNode交互，读取或写入数据

● 管理HDFS

#### 元数据（Metadata）

● 信息存放在NameNode内存当中
  ○ 包含：HDFS中文件及目录的基本属性信息（如拥有者、权限信息创建时间等）、文件有哪些block构成、以及block的位置存放信息。

● 元数据信息持久化

  ○ fsimage（元数据镜像检查点文件）

​    ■ fsimage文件是文件系统的元数据的持久性检查点，文件名上会记录对应的transaction ID

​    ■ 不会随文件系统的每个写操作而做持久化更新，因为写fsimage文件的速度非常慢

  ○ edits（编辑日志文件）

​    ■ 文件系统客户端执行写操作的时候，首先会被记录在edit log中

​    ■ 每个edit log文件通过前后缀记录当前操作的transaction ID̶

​    ■ 编辑日志会在每次写操作之后但尚未将成功代码返回给客户端时，被刷新和同步

  ○ block的位置信息并不会做持久化，仅仅只是在DataNode启动汇报给NameNode，存放在NameNode内存空间内。

### 高可用

- Journal集群

- - 负责存储edits编辑日志
  - 部署奇数（2N+1）台服务器

- 利用QJM实现元数据edits文件高可用

- - QJM机制（Quorum Journal Manager）
  - 只要保证Quorum（法定人数）数量的操作成功，就认为这是一次最终成功的操作

- QJM共享存储系统

- - 写edits的时候，只要超过半数（>=N+1）的JournalNode返回成功，就代表本次写入成功	
  - 最多可容忍N个JournalNode宕机

- - 基于Paxos算法实现

![img](https://cdn.nlark.com/yuque/0/2021/png/23034366/1638677783343-fbf3a2b3-da7d-4ea7-aa3f-9f11a6749b78.png)

#### 优点

- 高容错、高可用、高扩展

- - 数据冗余多副本，副本丢失后自动恢复
  - NameNode HA、安全模式

- - 10K节点规模

- 海量数据存储

- - 典型文件大小GB~TB，百万以上文件数量， PB以上数据规模

- 构建成本低、安全可靠

- - 构建在廉价的商用服务器上
  - 提供了容错和恢复机制

- 适合大规模离线批处理

- - 顺序数据访问
  - 数据位置暴露给计算框架

#### 缺点

- 不适合低延迟数据访问
- 不适合大量小文件存储

- - 元数据占用NameNode大量内存空间
  - 磁盘寻道时间超过读取时间

- 不支持并发写入：一个文件同时只能有一个写入者
- 不支持文件随机修改,仅支持追加写入



## HBase

### 概览

HBase - Hadoop Base 是一个高可靠、高性能的、面向列、可伸缩的分布式系统、利用HBase技术可在廉价PC Server上搭建大规模结构化存储集群。HBase是Google Bigtable的java开源实现，类似Google Bigtable利用GFS作为其文件存储系统，HBase利用Hadoop HDFS作为其文件存储系统，Google运行MapReduce来处理Bigtable中的海量数据，HBase同样利用Hadoop MapReduce来处理HBase中的海量数据，Google Bigtable利用Chubby作为协同服务，HBase利用Zookeeper作为协同服务。

### 功能

HBase的这些特点适用于数据海量(通常在TB级别以上)、持续增长的情况，存放一些结构比较简单的数据，如历史订单记录，日志数据，监控Metris数据等。
Facebook公司整理HBase集群使用的场景如下：

- 需要很高的写吞吐量
- 在大规模数据集中进行随机访问

- 方便的水平扩展
- 结构化和半结构化数据

- 不需要全部的关系数据库特性，例如交叉列、交叉表，事务，连接等等

HBase提供简单的基于Key值的快速查询能力，主要是前缀查询，条件查询和时间区间查询只能通过全表扫描获得。

HBase支持四种主要的数据操作，分别是Get/Scan/Put/Delete，其中Get和Scan代表数据查询，Put操作代表数据插入或更新（如果Put的RowKey不存在则为插入操作、否则为更新操作），特别需要注意的是HBase中更新操作并不是直接覆盖修改原数据，而是生成新的数据，新数据和原数据具有不同的版本（时间戳）；Delete操作执行数据删除，和数据更新操作相同，HBase执行数据删除并不会马上将数据从数据库中永久删除，而只是生成一条删除记录，最后在系统执行文件合并的时候再统一删除。

### 架构

![img](https://cdn.nlark.com/yuque/0/2021/png/23034366/1639286115365-2f48f638-243d-4daf-9234-4a8da66f3203.png?x-oss-process=image%2Fresize%2Cw_980%2Climit_0)

#### 角色

##### Client

Client提供了访问HBase的接口，并且维护了对应的cache来加速HBase的访问。

##### Zookeeper

对于 HBase 而言，Zookeeper 的作用是至关重要的。首先 Zookeeper 是作为 HBase Master 的 HA 解决方案。也就是说，是 Zookeeper 保证了至少有一个 HBase Master 处于运行状态。并且 Zookeeper 负责 Region 和 Region Server 的注册。其实 Zookeeper 发展到目前为止，已经成为了分布式大数据框架中容错性的标准框架。不光是 HBase，几乎所有的分布式大数据相关的开源框架，都依赖于 Zookeeper 实现 HA。

Zookeeper的特性，没有高吞吐，没有大容量，但是能提供无与伦比的可靠性。

##### HMaster

HBase Master 用于协调多个 Region Server，侦测各个 Region Server 之间的状态，并平衡 Region Server 之间的负载。HBase Master 还有一个职责就是负责分配 Region 给 Region Server。HBase 允许多个 Master 节点共存，但是这需要 Zookeeper 的帮助。不过当多个 Master 节点共存时，只有一个 Master 是提供服务的，其他的 Master 节点处于待命的状态。当正在工作的 Master 节点宕机时，其他的 Master 则会接管 HBase 的集群。

##### Region Server

对于一个 Region Server 而言，其包括了多个 Region。Region Server 的作用只是管理表格，以及实现读写操作。Client 直接连接 Region Server，并通信获取 HBase 中的数据。对于 Region 而言，则是真实存放 HBase 数据的地方，也就说 Region 是 HBase 可用性和分布式的基本单位。如果当一个表数据量很大，并由多个 CF 组成时，那么表的数据将存放在多个 Region 之间，并且在每个 Region 中会关联多个存储的单元（Store）。

HMaster和HRegionServer构成主从架构，可以对标NameNode和DataNode

#### 应用实践

##### 表的设计

###### Rowkey设计

HBase是采用Key-Value形式的列存储，rowkey是HBase的key-value存储中的key，所以rowkey的设计是非常重要，直接影响到HBase的性能。结合业务的特点，并考虑高频查询，尽可能的将数据打散到整个集群。
Rowkey是一个二进制码流，Rowkey的长度被很多开发者建议说设计在10~100个字节，不过建议是越短越好，不要超过16个字节。原因如下：

- 数据的持久化文件HFile中是按照KeyValue存储的，如果Rowkey过长比如100个字节，1000万列数据光Rowkey就要占用100*1000万=10亿个字节，将近1G数据，这会极大影响HFile的存储效率；
- MemStore将缓存部分数据到内存，如果Rowkey字段过长内存的有效利用率会降低，系统将无法缓存更多的数据，这会降低检索效率。因此Rowkey的字节长度越短越好。

- 目前操作系统是都是64位系统，内存8字节对齐。控制在16个字节，8字节的整数倍利用操作系统的最佳特性。

内部规约：

- RowKey不宜过长，一般为10-100bytes，控制在16bytes内最佳
- RowKey前面一定要加hash，特别对于数据量或请求量大的表

- 列族个数不能过多，最多不要超过3个，各个列族的数据量尽量控制在一个量级
- 列族名也不要过长，一般为1-3字符

- 大表最好加上TTL，减轻存储压力
- 对于获取整行请求的表，字段的个数不能过多，建议把多个字段合成一个json对象

- 单个Cell的对象不宜过大，默认最大10MB



RowKey散列原则：

如果Rowkey是按时间戳的方式递增，不要将时间放在二进制码的前面，建议将Rowkey的高位作为散列字段，由程序循环生成，低位放时间字段，这样将提高数据均衡分布在每个Regionserver实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息将产生所有新数据都在一个RegionServer上堆积的热点现象，这样在做数据检索的时候负载将会集中在个别RegionServer，降低查询效率。

RowKey唯一原则：

必须在设计上保证其唯一性。
当客户端需要频繁的写一张表，随机的 RowKey 会获得更好的性能。
当客户端需要频繁的读一张表，有序的 RowKey 则会获得更好的性能。
对于时间连续的数据（例如 log），有序的 RowKey 会很方便查询一段时间的数据（Scan 操作）。

###### 列簇设计

HBase的表设计时，根据不同需求有不同选择，需要做在线查询的数据表，尽量不要设计多个列簇，我们知道，不同的列簇在存储上是被分开的，多列簇设计会造成在数据查询的时候读取更多的文件，从而消耗更多的I/O。

###### TTL设计

选择合适的数据过期时间也是表设计中需要注意的一点，HBase中允许列簇定义数据过期时间，数据一旦超过过期时间，可以被major compact进行清理。大量无用历史数据的残余，会造成region体积增大，影响查询效率。

##### Region设计

一般地，region不宜设计成很大，除非应用对阶段性性能要求很多，但是在将来运行一段时间可以接受停服处理。region过大会导致major compact调用的周期变长，而单次major compact的时间也相应变长。major compact对底层I/O会造成压力，长时间的compact操作可能会影响数据的flush，compact的周期变长会导致许多删除或者过期的数据不能被及时清理，对数据的读取速度等都有影响。

相反，小的region意味着major compact会相对频繁，但是由于region比较小，major compact的相对时间较快，而且相对较多的major compact操作，会加速过期数据的清理。

当然，小region的设计意味着更多的region split风险，region容量过小，在数据量达到上限后，region需要进行split来拆分，其实split操作在整个HBase运行过程中，是被不怎么希望出现的，因为一旦发生split，涉及到数据的重组，region的再分配等一系列问题。所以我们在设计之初就需要考虑到这些问题，尽量避免region的运行过程中发生split。

HBase可以通过在表创建的时候进行region的预分配来解决运行过程中region的split产生，在表设计的时候，预先分配足够多的region数，在region达到上限前，至少有部分数据会过期，通过major compact进行清理后， region的数据量始终维持在一个平衡状态。

region数量的设计还需要考虑内存上的限制，通过前面的介绍我们知道每个region都有memstore，memstore的数量与region数量和region下列簇的数量成正比：

一个RegionServer下memstore内存消耗：

Memory = memstore大小 * region数量 * 列簇数量

如果不进行前期数据量估算和region的预分配，通过不断的split产生新的region，容易导致因为内存不足而出现OOM现象。

##### WAL日志

WAL日志是HBase保证数据持久性的一种机制，类似于MySQL的redo log，一般默认是开启WAL日志的，这样可以在节点故障时保证数据的一致性。有些应用为了更高的吞吐率会牺牲数据一致性，手动关闭WAL写入。WAL日志相关参数可以在客户端设置，可选的设置有：

- ASYNC_WAL：异步地写入WAL日志，可能会造成部分数据丢失。
- SYNC_WAL：同步将数据写入日志文件中，需要注意的是数据只是被写入文件系统中，并没有真正落盘，默认。

- FSYNC_WAL：同步将数据写入日志文件并强制落盘，可以保证数据不会丢失，但是性能相对比较差。
- SKIP_WAL：只写缓存，不写WAL日志，性能最高的写入方式，但节点挂掉会造成部分数据丢失。

##### 核心配置参数调优

###### Region相关

**hbase.hregion.max.filesize**：默认10G，简单理解为Region中任意HStore所有文件大小总和大于该值就会进行分裂。

解读：实际生产环境中该值不建议太大，也不能太小。太大会导致系统后台执行compaction消耗大量系统资源，一定程度上影响业务响应；太小会导致Region分裂比较频繁（分裂本身其实对业务读写会有一定影响），另外单个RegionServer中必然存在大量Region，太多Region会消耗大量维护资源，并且在rs下线迁移时比较费劲。综合考虑，建议线上设置为50G～80G左右。



###### BlockCache相关

RS内存在20G以内的就选择LRUBlockCache，大于20G的就选择BucketCache中的Offheap模式。接下来所有的相关配置都基于BucketCache的offheap模型进行说明。



**file.block.cache.size**：默认0.4，该值用来设置LRUBlockCache的内存大小，0.4表示JVM内存的40%。

解读：当前HBase系统默认采用LRUBlockCache策略，BlockCache大小和Memstore大小均为JVM的40%。但对于BucketCache策略来讲，Cache分为了两层，L1采用LRUBlockCache，主要存储HFile中的元数据Block，L2采用BucketCache，主要存储业务数据Block。因为只用来存储元数据Block，所以只需要设置很小的Cache即可。建议线上设置为0.05～0.1左右。

**hbase.bucketcache.ioengine**：offheap

**hbase.bucketcache.size**：堆外存大小，设置多大就看自己的物理内存大小喽

##### Memstore相关

**hbase.hregion.memstore.flush.size**：默认128M（134217728），memstore大于该阈值就会触发flush。如果当前系统flush比较频繁，并且内存资源比较充足，可以适当将该值调整为256M。

**hbase.hregion.memstore.block.multiplier**：默认4，表示一旦某region中所有写入memstore的数据大小总和达到或超过阈值hbase.hregion.memstore.block.multiplier * hbase.hregion.memstore.flush.size，就会执行flush操作，并抛出RegionTooBusyException异常。

解读：该值在1.x版本默认值为2，比较小，为了保险起见会将其修改为5。而当前1.x版本默认值已经为4，通常不会有任何问题。如果日志中出现类似”Above memstore limit, regionName = ***, server=***,memstoreSizse=***,blockingMemstoreSize=***”，就需要考虑修改该参数了。

**hbase.regionserver.global.memstore.size**：默认0.4，表示整个RegionServer上所有写入memstore的数据大小总和不能超过该阈值，否则会阻塞所有写入请求并强制执行flush操作，直至总memstore数据大小降到hbase.regionserver.global.memstore.lowerLimit以下。

解读：该值在offheap模式下需要配置为0.6～0.65。一旦写入出现阻塞，立马查看日志定位“Blocking update on ***: the global memstore size *** is >= than blocking *** size”。一般情况下不会出现这类异常，如果出现需要明确是不是region数目太多、单表列族设计太多。

**hbase.regionserver.global.memstore.lowerLimit**：默认0.95。不需要修改。

**hbase.regionserver.optionalcacheflushinterval**：默认1h（3600000），hbase会起一个线程定期flush所有memstore，时间间隔就是该值配置。

解读：生产线上该值建议设大，比如10h。因为很多场景下1小时flush一次会导致产生很多小文件，一方面导致flush比较频繁，一方面导致小文件很多，影响随机读性能。因此建议设置较大值。



###### Compaction相关

compaction模块主要用来合并小文件，删除过期数据、deleted数据等。涉及参数较多，对于系统读写性能影响也很重要，下面主要介绍部分比较核心的参数。

**hbase.hstore.compactionThreshold：**默认为3，compaction的触发条件之一，当store中文件数超过该阈值就会触发compaction。通常建议生产线上写入qps较高的系统调高该值，比如5～10之间。

**hbase.hstore.compaction.max：**默认为10，最多可以参与minor compaction的文件数。该值通常设置为**hbase.hstore.compactionThreshold略的2～3倍。**

**hbase.regionserver.thread.compaction.throttle：**默认为2G，评估单个compaction为small或者large的判断依据。为了防止large compaction长时间执行阻塞其他small compaction，hbase将这两种compaction进行了分离处理，每种compaction会分配独立的线程池。

**hbase.regionserver.thread.compaction.large/small：**默认为1，large和small compaction的处理线程数。生产线上建议设置为5，强烈不建议再调太大（比如10），否则会出现性能下降问题。

**hbase.hstore.blockingStoreFiles：**默认为10，表示一旦某个store中文件数大于该阈值，就会导致所有更新阻塞。生产线上建议设置该值为100，避免出现阻塞更新，一旦发现日志中打印’*** too many store files***’，就要查看该值是否设置正确。

**hbase.hregion.majorcompaction：**默认为1周（1000*60*60*24*7），表示major compaction的触发周期。生产线上建议大表major compaction手动执行，需要将此参数设置为0，即关闭自动触发机制。



###### HLog相关

**hbase.regionserver.maxlogs**：默认为32，region flush的触发条件之一，wal日志文件总数超过该阈值就会强制执行flush操作。该默认值对于很多集群来说太小，生产线上具体设置参考HBASE-14951

**hbase.regionserver.hlog.splitlog.writer.threads**：默认为3，regionserver恢复数据时日志按照region切分之后写入buffer，重新写入hdfs的线程数。生产环境因为region个数普遍较多，为了加速数据恢复，建议设置为10。



###### Call Queue相关

**hbase.regionserver.handler.count**：默认为30，服务器端用来处理用户请求的线程数。生产线上通常需要将该值调到100～200。

解读：response time = queue time + service time，用户关心的请求响应时间由两部分构成，优化系统需要经常关注queue time，如果用户请求排队时间很长，首要关注的问题就是hbase.regionserver.handler.count**是否没有调整。**

**hbase.ipc.server.callqueue.handler.factor :** 默认为0，服务器端设置队列个数，假如该值为0.1，那么服务器就会设置handler.count * 0.1 = 30 * 0.1 = 3个队列。

**hbase.ipc.server.callqueue.read.ratio** : 默认为0，服务器端设置读写业务分别占用的队列百分比以及handler百分比。假如该值为0.5，表示读写各占一半队列，同时各占一半handler。

**hbase.ipc.server.call.queue.scan.ratio**：默认为0，服务器端为了将get和scan隔离设置了该参数。

######  其他

**hbase.online.schema.update.enable**：默认为false，表示更新表schema的时候不再需要先disable再enable，直接在线更新。该参数在HBase 2.0之后将会默认为true。生产线上建议设置为true。

**hbase.quota.enabled**：默认为false，表示是否开启quota功能，quota功能主要用来限制用户/表的QPS，起到限流作用。生产线上建议设置为true。

**hbase.snapshot.enabled**：默认为false，表示是否开启snapshot功能，snapshot功能主要用来备份HBase数据。生产线上建议设置为true。

**zookeeper.session.timeout**：默认180s，表示zookeeper客户端与服务器端session超时时间，超时之后RegionServer将会被踢出集群。

解读：有两点需要重点关注，其一是该值需要与Zookeeper服务器端session相关参数一同设置才会生效，一味的将该值增大而不修改ZK服务端参数，可能并不会实际生效。其二是通常情况下离线集群可以将该值设置较大，在线业务需要根据业务对延迟的容忍度考虑设置。

**hbase.zookeeper.useMulti**：默认为false，表示是否开启zookeeper的multi-update功能，该功能在某些场景下可以加速批量请求完成，而且可以有效防止部分异常问题。生产线上建议设置为true。注意设置为true的前提是Zookeeper服务端的版本在3.4以上，否则会出现zk客户端夯住的情况。

**hbase.coprocessor.master.classes**：生产线上建议设置org.apache.hadoop.hbase.security.access.AccessController，可以使用grant命令对namespace\table\cf设置访问权限。

**hbase.coprocessor.region.classes**：生产线上建议设置org.apache.hadoop.hbase.security.token.TokenProvider,org.apache.hadoop.hbase.security.access.AccessController，同上。

##### 业务场景

HBase虽然是一个分布式系统（按照RowKey范围划分为多个region），但只具有数据的高可用，不具有读写的高可用。所谓的数据高可用就是依靠底层的HDFS副本机制保证，这已经是非常成熟的。当一个Region Server宕机时，该节点上的Region需要通过先回放WAL日志，才能重新分配给其它节点上线，回放日志是一个比较耗时的操作，这也就带来了恢复期间读写的不可用。2.X的版本对故障恢复有很大的优化，还引入了Region Replica的概念，但也只能保证读高可用，现在还不太成熟并没有线上使用。所以，强烈建议业务不要强依赖HBase，要有一定的兜底和熔断措施。
